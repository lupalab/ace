import tensorflow as tf

layers = tf.keras.layers


class ProposalNetwork(layers.Layer):
    """A proposal network used by ACE.

    This network accepts the observed features and corresponding mask and outputs
    parameters for a Gaussian mixture for all unobserved features. It can also
    optionally output a "context vector" for each feature that is used to enable weight
    sharing with the energy network.

    This network expects as input a list where the first element is the observed
    features and the second element is the mask.

    Args:
        num_mixture_components: The number of components in each mixture model.
        num_context_units: Optional. The dimensionality of the context vectors.
        num_residual_blocks: The number of residual blocks.
        hidden_units: The number of hidden units in each layer.
        activation: The activation function used in the network.
        dropout: Optional. The dropout rate.
        name: Name of the layer.
    """

    def __init__(
        self,
        num_mixture_components,
        num_context_units=None,
        num_residual_blocks=4,
        hidden_units=256,
        activation="relu",
        dropout=None,
        name="proposal_network",
        **kwargs
    ):
        super().__init__(name=name, **kwargs)

        self._num_residual_blocks = num_residual_blocks
        self._hidden_units = hidden_units
        self._dropout = dropout

        self._output_units = 3 * num_mixture_components + (num_context_units or 0)

        self._hidden_layers = None
        self._output_layer = None
        self._activation = layers.Activation(activation)

    def build(self, input_shape):
        self._hidden_layers = [
            layers.Dense(self._hidden_units)
            for _ in range(2 * self._num_residual_blocks + 1)
        ]
        self._output_layer = layers.Dense(self._output_units * input_shape[0][-1])

    def call(self, inputs, training=False):
        x_o, mask = inputs

        h = self._hidden_layers[0](tf.concat([x_o, mask], axis=-1))

        for block in range(self._num_residual_blocks):
            res = self._activation(h)

            res = self._hidden_layers[block * 2 + 1](res)
            res = self._activation(res)

            if self._dropout and training:
                res = tf.nn.dropout(res, rate=self._dropout)

            res = self._hidden_layers[block * 2 + 2](res)

            h += res

        h = self._activation(h)
        output = self._output_layer(tf.concat([h, mask], axis=-1))

        return tf.reshape(output, [-1, x_o.shape[-1], self._output_units])


class EnergyNetwork(layers.Layer):
    """An energy network used by ACE.

    This network accepts the observed features (and mask), which represents the
    conditioning information, and a single unobserved feature for which an energy should
    be computed. We compute energies for many unobserved features by stacking them in a
    batch.

    Specifically, this network expects as input a list that contains the following:
        x_u_i: The value of the unobserved feature. A tensor with shape `[batch_size]`.
        u_i: The index of the unobserved feature. A tensor with shape `[batch_size]`.
        x_o: The observed features. A tensor with shape `[batch_size, num_features]`.
        mask: The mask that indicates observed features.
            A tensor with shape `[batch_size, num_features]`.
        context: Optional. The context vector generated by the proposal network. Should
            be None if context vectors are not being used. A tensor with shape
            `[batch_size, num_context_units]`.


    Args:
        num_residual_blocks: The number of residual blocks.
        hidden_units: The number of hidden units in each layer.
        activation: The activation function used in the network.
        dropout: Optional. The dropout rate.
        name: Name of the layer.
    """

    def __init__(
        self,
        num_residual_blocks=4,
        hidden_units=128,
        activation="relu",
        dropout=None,
        name="energy_network",
        **kwargs
    ):
        super().__init__(name=name, **kwargs)

        self._num_residual_blocks = num_residual_blocks
        self._hidden_units = hidden_units
        self._activation = activation
        self._dropout = dropout

        self._hidden_layers = None
        self._output_layer = None
        self._activation = layers.Activation(activation)

    def build(self, input_shape):
        self._hidden_layers = [
            layers.Dense(self._hidden_units)
            for _ in range(2 * self._num_residual_blocks + 1)
        ]
        self._output_layer = layers.Dense(1)

    def call(self, inputs, training=False):
        x_u_i, u_i, x_o, mask, context = inputs

        u_i_one_hot = tf.one_hot(u_i, x_o.shape[-1])

        if context is not None:
            h = tf.concat(
                [x_u_i[..., tf.newaxis], u_i_one_hot, x_o, mask, context], axis=-1
            )
        else:
            h = tf.concat([x_u_i[..., tf.newaxis], u_i_one_hot, x_o, mask], axis=-1)

        h = self._hidden_layers[0](h)

        for block in range(self._num_residual_blocks):
            res = self._activation(h)

            res = self._hidden_layers[block * 2 + 1](res)
            res = self._activation(res)

            if self._dropout and training:
                res = tf.nn.dropout(res, rate=self._dropout)

            res = self._hidden_layers[block * 2 + 2](res)

            h += res

        h = self._activation(h)

        output = self._output_layer(tf.concat([h, mask, x_o], axis=-1))
        output = -tf.nn.softplus(output)
        return tf.squeeze(output)
